# lightning.pytorch==2.2.1
seed_everything: true

paths:
  dataset_dir: "data/dataset"

data:
  class_path: src.datamodules.cryoem_map_datamodule.CryoemDensityMapDataModule
  init_args:
    dataset_dir: ${paths.dataset_dir}
    train_max_samples: 0
    val_max_samples: 0
    batch_size: 4
    num_workers: 8

model:
  class_path: src.models.CryoTEN.model.CryoTENLitModule
  init_args:
    optimizer:
      class_path: torch.optim.Adam
      init_args:
        lr: 0.0005
        weight_decay: 0.0

    scheduler:
      class_path: torch.optim.lr_scheduler.ReduceLROnPlateau
      init_args:
        mode: min
        factor: 0.5
        patience: 2
        min_lr: 0.00001

    net:
      class_path: src.models.CryoTEN.network.CryoTEN
      init_args:
        in_channels: 1
        out_channels: 1
        img_size: 48
        feature_size: 16
        hidden_size: 256
        num_heads: 4
        depths: [3, 3, 3, 3]
        dims: [32, 64, 128, 256]
        pos_embed: "perceptron"
        norm_name: "instance"
        dropout_rate: 0.0
        do_ds: False

trainer:
  accelerator: gpu
  strategy: auto
  devices: -1
  num_nodes: 1
  max_epochs: 300
  min_epochs: 1
  check_val_every_n_epoch: 5
  num_sanity_val_steps: 0
  log_every_n_steps: 10
  deterministic: false
  detect_anomaly: false
  sync_batchnorm: true
  default_root_dir: null

  callbacks:
    - class_path: lightning.pytorch.callbacks.ModelCheckpoint
      init_args:
        dirpath: null # directory to save the model file
        filename: "epoch_{epoch:03d}" # checkpoint filename
        monitor: null # name of the logged metric which determines when model is improving
        save_last: True # additionally always save an exact copy of the last checkpoint to a file last.ckpt
        save_top_k: -1 # save k best models (determined by above metric). if -1, it will run for every epoch!?
        mode: "min" # "max" means higher metric value is better, can be also "min"
        auto_insert_metric_name: False # when True, the checkpoints filenames will contain the metric name
        save_weights_only: False # if True, then only the modelâ€™s weights will be saved
        every_n_epochs: 1 # number of epochs between checkpoints
        save_on_train_epoch_end: True # whether to run checkpointing at the end of the training epoch or the end of validation

    - class_path: lightning.pytorch.callbacks.RichModelSummary
      init_args:
        max_depth: -1
    
    # - class_path: lightning.pytorch.callbacks.RichProgressBar

  logger:
    - class_path: lightning.pytorch.loggers.WandbLogger
      init_args:
        id: null # pass correct id to resume experiment!
        # name: "" # name of the run (normally generated by wandb). use --trainer.logger.name to specify it.
        save_dir: "logs/" # directory to save the wandb logs
        project: "enhance-cryoem-map"
        entity: "joelselvaraj" # set to name of your wandb team

ckpt_path: null
